# Python Libraries, Frameworks and packages.

## Pytorch
The model and data pre-processing are implemented using `PyTorch` as an aspect, PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library (Yasar, 2022).

### How PyTorch was used :
(Pytorch.org, 2024)

- `Create_Datasets.py`
    - Imported the function `random_split(...)` from `torch.utils.data`, this functon is used to randomly split the dataset into     non-overlapping new datasets of given lengths, namely into validation, test and training datasets.


- `LSTM.py`
    - Imported `torch.nn` as `nn` and used the following functions: 
        - `nn.Embedding(...)`
        - `nn.LSTM(...)`
        - `nn.Linear(...)`
    - `nn.Embedding(...)`, used to make vectors of the words/tokens from the articles, so the model can map words or tokens to learnable vector representations.
    - `nn.LSTM(...)`, initializes a multi-layer LSTM Long Short-Term Memory network to process the sequential data.
    - `nn.Linear(...)`, creates a linear layer to map the output of an LSTM or any hidden layer to the desired output size, typically the number of classes or target values.


- `Train.py`
    - Imported `torch`, `torch.nn` as `nn`, `import torch.optim as optim` and `torch.utils.data import DataLoader, TensorDataset`
    - `torch` used by...
    - `torch.nn` used `nn.CrossEntropyLoss()` to initialize a cross-entropy loss function.
    - `torch.optim` initliazes the optimizer... (subject to change)
    - `torch.utils.data` made use of `TensorDataset` creates a PyTorch TensorDataset, to wrap the torch.Tensor objects (inputs and labels) into a dataset so they can be accessed together one sample at a time, later the samples are to be loaded into `DataLoader`.



- To use PyTorch to make an LSTM model the following steps are taken:
    - `import torch`
    - `import torch.nn`
    

It is particulary relevant to our chosen `LSTM` model because PyTorch offers support for sequential models, such as LSTMs, with easy to use abstractions like `nn.LSTM`, dynamic computational graphs, and integration with GPU acceleration. These features make PyTorch well-suited for handling natural language data and efficiently training sequence models.

## Natural Language Toolkit (NLTK)

- Tokenization  
NLTK's tokenization functions were employed at two levels:  
    - **Sentence Tokenization**: `sent_tokenize()` splits documents into sentences  
    - **Word Tokenization**: `word_tokenize()` splits sentences into words  

- Stopword Removal  
    - Used NLTK's English stopwords list (`stopwords.words('english')`) to filter common words.

- Lemmatization  
    - NLTK's `WordNetLemmatizer` reduced words to their base forms (lemmas).

- NLTK Data Downloads  
    Downloaded these NLTK data packages:  
    - `stopwords`  
    - `wordnet` (for lemmatization)  
    - `punkt` (for tokenization)  

- Preprocessing Pipeline

#### Text Cleaning Steps  
1. **Case Normalization**: Convert text to lowercase  
2. **Special Character Removal**: Regex removes non-alphabetic chars  
3. **Stopword Removal**: Filters common stopwords  
4. **Lemmatization**: Reduces words to base forms  
5. **Special Tokens**: Adds `<eos>` markers  

- Vocabulary Construction  
    1. Built vocabulary from words appearing ≥3 times (`MIN_VOCAB_FREQ`)  
    2. Included special tokens:  
    - `<pad>` (padding)  
    - `<eos>` (end-of-sentence)  
    - `<unk>` (unknown words)  

- Data Encoding  
    1. Encoded text using vocabulary indices  
    2. Handled unknown words with `<unk>`  
    3. Padded/truncated to fixed length (`MAX_ARTICLE_LEN`)

NLTK was critical in preparing the news dataset for training the LSTM model, which classifies articles as real or fake. Since LSTMs are sensitive to the structure and quality of input sequences, effective preprocessing ensures that the model learns meaningful patterns rather than noise.

Overall, this NLTK used in the preprocessing pipeline enhances the quality and consistency of the text data, enabling the LSTM model to better learn the linguistic and semantic patterns that differentiate the real news from fake news.



## Scikit-learn

Scikit-learn's module was integrated to evaluate the performance of an LSTM model for fake news classification.


- Performance Metrics

``` python
from sklearn.metrics import (
    accuracy_score,
    classification_report, 
    confusion_matrix
) 
```

- The dataset splitting strategy using scikit-learn `train_test_split` function to create training, validation, and test sets for model development.

```python
from sklearn.model_selection import train_test_split

def Split_Dataset(dataset):
    """Splits dataset into training (60%), validation (20%), and test (20%) sets
    
    Args:
        dataset: Pandas DataFrame containing text and label columns
        
    Returns:
        Tuple of (training_data, validation_data, test_data)
    """
    # First split: 60% training, 40% temp holdout
    training_data, temp_data = train_test_split(
        dataset, 
        test_size=0.4, 
        random_state=42, 
        stratify=dataset['label']
    )
    
    # Second split: 50/50 split of temp data → 20% validation, 20% test
    validation_data, test_data = train_test_split(
        temp_data, 
        test_size=0.5, 
        random_state=42, 
        stratify=temp_data['label']
    )
    
    return training_data, validation_data, test_data
    
```


##### Key Features

- Maintains original class distribution using stratify parameter which is important for imbalanced datasets.Ensures representative samples of both classes ('Fake'/'Real') in all splits.

- Reproducible Splits
    - Fixed `random_state=42` guarantees:

        - Same splits across different runs
        - Consistent model evaluation

Scikit-learn was used to support the evaluation and data preparation stages of the LSTM-based fake news classifier. Its `train_test_split` function enabled systematic splitting of the dataset into training (60%), validation (20%), and test (20%) sets. Importantly, the use of the `stratify` parameter preserved the original class distribution across all subsets, which is important when working with potentially imbalanced datasets like fake vs. real news.

In addition, setting a fixed `random_state` ensured reproducibility, enabling consistent model training and fair comparisons across experiments.

For performance evaluation, Scikit-learn's metric functions (`accuracy_score`, `classification_report`, `confusion_matrix`) provided an analysis of the model’s predictions. These metrics helped quantify not just overall accuracy, but also class-specific precision, recall, and F1-scores.

With Scikit-learn, the pipeline ensures reliable dataset handling and standardized evaluation, both of which are essential for developing trustworthy models in fake news detection tasks.
