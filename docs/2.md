# Python Libraries, Frameworks and packages.

## Pytorch
The model and data pre-processing are implemented using `PyTorch` as an aspect, PyTorch is an open source machine learning (ML) framework based on the Python programming language and the Torch library (Yasar, 2022).

### How PyTorch was used :
(Pytorch.org, 2024)

- `Create_Datasets.py`
    - Imported the function `random_split(...)` from `torch.utils.data`, this functon is used to randomly split the dataset into     non-overlapping new datasets of given lengths, namely into validation, test and training datasets.


- `LSTM.py`
    - Imported `torch.nn` as `nn` and used the following functions: 
        - `nn.Embedding(...)`
        - `nn.LSTM(...)`
        - `nn.Linear(...)`
    - `nn.Embedding(...)`, used to make vectors of the words/tokens from the articles, so the model can map words or tokens to learnable vector representations.
    - `nn.LSTM(...)`, initializes a multi-layer LSTM Long Short-Term Memory network to process the sequential data.
    - `nn.Linear(...)`, creates a linear layer to map the output of an LSTM or any hidden layer to the desired output size, typically the number of classes or target values.


- `Train.py`
    - Imported `torch`, `torch.nn` as `nn`, `import torch.optim as optim` and `torch.utils.data import DataLoader, TensorDataset`
    - `torch` used by...
    - `torch.nn` used `nn.CrossEntropyLoss()` to initialize a cross-entropy loss function.
    - `torch.optim` initliazes the optimizer... (subject to change)
    - `torch.utils.data` made use of `TensorDataset` creates a PyTorch TensorDataset, to wrap the torch.Tensor objects (inputs and labels) into a dataset so they can be accessed together one sample at a time, later the samples are to be loaded into `DataLoader`.



- To use PyTorch to make an LSTM model the following steps are taken:
    - `import torch`
    - `import torch.nn`
    

It is particulary relevant to our chosen `LSTM` model because 

## Natural Language Toolkit (NLTK)

- Tokenization  
NLTK's tokenization functions were employed at two levels:  
    - **Sentence Tokenization**: `sent_tokenize()` splits documents into sentences  
    - **Word Tokenization**: `word_tokenize()` splits sentences into words  

- Stopword Removal  
    - Used NLTK's English stopwords list (`stopwords.words('english')`) to filter common words.

- Lemmatization  
    - NLTK's `WordNetLemmatizer` reduced words to their base forms (lemmas).

- NLTK Data Downloads  
    Downloaded these NLTK data packages:  
    - `stopwords`  
    - `wordnet` (for lemmatization)  
    - `punkt` (for tokenization)  

- Preprocessing Pipeline

#### Text Cleaning Steps  
1. **Case Normalization**: Convert text to lowercase  
2. **Special Character Removal**: Regex removes non-alphabetic chars  
3. **Stopword Removal**: Filters common stopwords  
4. **Lemmatization**: Reduces words to base forms  
5. **Special Tokens**: Adds `<eos>` markers  

- Vocabulary Construction  
    1. Built vocabulary from words appearing â‰¥3 times (`MIN_VOCAB_FREQ`)  
    2. Included special tokens:  
    - `<pad>` (padding)  
    - `<eos>` (end-of-sentence)  
    - `<unk>` (unknown words)  

- Data Encoding  
    1. Encoded text using vocabulary indices  
    2. Handled unknown words with `<unk>`  
    3. Padded/truncated to fixed length (`MAX_ARTICLE_LEN`)  


## Pandas

