# Preprocessing

 To prepare the data for training a text classification model, we applied a series of Natural Language Processing preprocessing steps. These steps were implemented in Python using the NLTK library and were designed to reduce noise, standardize the text, remove unnecessary variation and convert raw language into embeddings (the real-valued vectors consumed by the LSTM).

## Data Splitting
**Before the vocabulary filtering, the dataset was split using SKlearn into:**
 - 60% training set – for model learning.
 - 20% validation set – for hyperparameter tuning and model selection.
 - 20% test set – for final performance evaluation.

 ![alt text](media/pi.png)

## Preprocessing pipeline:
**1. Dataset Construction and Removing Duplicates:**

- Combined Fake.csv and True.csv datasets into `Create_Dataset`, added binary labels (`0` = fake, `1` = real).
- Removed rows with empty text and removed duplicate articles.
- Shuffled the resulting dataset.
- Originally, we had 44 898 articles, after removing 6 252 duplicates and 631 empty-text rows, we ended up with 38 646 articles.

**2. Lowercasing and Data Cleaning:**

In `Preprocessing.py`, all text was converted to lowercase. Special characters, punctuation, and non-alphabetic symbols were removed via regex. 

**3. Sentence and Word Tokenization:**

Articles were split into individual sentences, and each sentence was further tokenized into words using NLTK's `word_tokenize` function.

**4. Stopword Removal:**

Common English stopwords (e.g., "the", "and", "in") were removed to eliminate low-value words.

**5. Lemmatization:**

Remaining words were lemmatized to reduce each word to its base form (e.g., "running" becomes "run").

**6. Special Tokens:**

We added an end of sentence token `<eos>` to each sentence. This helped the model recognize where sentences began and ended - an important step in training the model to understand sentence structure and anchor the position of key information.

**7. Vocabulary Filtering:**

We built an enumerated vocabulary from the cleaned tokens, keeping only tokens that occurred at least 3 times in the training dataset. This ensures the model learns to generalise from known patterns on seen data and can successfully handle unseen data during validation and testing.

**8. Truncation and Padding:**

Each article was then condensed to a maximum length of 256 tokens. The beginning of a news article typically contains the most important content. LSTMs struggle with long sequences as they increase learning complexity, resulting in overfitting and making training noisier. All token sequences less than the maximum length were padded so that every article was the same length - necessity for batch processing and consistent input dimensions for the LSTM.

**9. Integer Encoding:**

LSTMs require numeric inputs that act as lookup keys for the embedding layer. The processed data was encoded as integer sequences using the vocabulary's enumeration. OOV (out-of-vocabulary) words were replaced with an `<unk>` (unknown) token.


We used NLTK for linguistic preprocessing (tokenization, stopword removal, and lemmatization)
In addition to these steps, we implemented efficient data storage. The final encoded dataset and vocabulary were saved as .pkl files in the Pickled_data folder to keep file sizes small and avoid exceeding GitHub's 100MB commit limit. Human-readable .txt files—containing the unprocessed text, vocabulary, and encoded sequences—were saved in a Readables folder, which is excluded from version control. These preprocessing steps not only standardized and cleaned the text but also contributed to noise reduction, dimensionality control (through a filtered vocabulary), and semantic consistency (via lemmatization). As a result, the model was able to train more efficiently and generalize better to unseen data.